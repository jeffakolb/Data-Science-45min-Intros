{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing collections (Part Two)\n",
    "\n",
    "* Motivation\n",
    "* Review Part I\n",
    "* Describe what we want\n",
    "* Kendall's tau\n",
    "* Rank-biased overlap\n",
    "* Apply to data\n",
    "\n",
    "Inspired by \"A Similarity Measure for Indefinite Rankings\", http://www.williamwebber.com/research/papers/wmz10_tois.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "We ofter want to programatically describe how two Twitter corpora are difference or the same. A common method for doing this is to count things and rank them by their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import time\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import nltk.tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "import twitter\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Part I\n",
    "\n",
    "## Set comparision\n",
    "\n",
    "* Looked at intersection and union\n",
    "\n",
    "## List comparision \n",
    "\n",
    "* Looked at Pearson's correlation coefficient\n",
    "\n",
    "## Ordinal (rank) comparison\n",
    "\n",
    "* Pulled tweets from users with 'mom' in bio. \n",
    "* created exact and approximate term frequency distributions of 1-grams in tweet bodies\n",
    "* Kendall's tau coefficient compares exact and approximate rankings\n",
    "\n",
    "## Over/under indexing\n",
    "\n",
    "* Pulled tweets from users with 'dad' in bio\n",
    "* Created exact term frequency distributions for 'mom' and 'dad' tweet corpora\n",
    "* Attemped to find a function that de-emphasizes common terms and emphasizes un-shared, highly ranked terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# So what do we want to do?\n",
    "\n",
    "We want to compare two lists of objects and their counts. This is a common need\n",
    "when comparing counts of hashtags, n-grams, locations, etc.\n",
    "\n",
    "We want the ability to act on lists that are:\n",
    "* _non-conjoint_: lists may contain different elements\n",
    "* _incomplete_: all elements in the list are not present or not analyzed\n",
    "* _indefinite_: the cutoff for the incomplete list is essentially arbitrary\n",
    "\n",
    "We also want to apply _weighting_: where the comparison emphasizes elements with highest counts.\n",
    "\n",
    "For output, we want to:\n",
    "* calculate a similarity score\n",
    "* highlight differences (save for next time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kendall's tau orrelation coefficient, again\n",
    "\n",
    "Kendall's tau is rank correlation coefficient that compares two sequences of _rankings_. \n",
    "Specifically, it is a function of the number of concordant and the number of discordant pairs.\n",
    "\n",
    "In our case, the data might look something like:\n",
    "\n",
    "ngram | rank in corpus 1 | rank in corpus 2 \n",
    "--- | --- | ---\n",
    "dad | 1 | 2\n",
    "parent | 2 | 3\n",
    "lol | 3 | 1\n",
    "know | 4 | 4\n",
    "\n",
    "The 'dad-parent' pair is concordant (C) because `1>2` and `2>3`, while the 'parent-lol' pair\n",
    "is discordant (D), because `2>3` but `3<1`. The 'dad-lol' pair is discordant, while the 'lol-know',\n",
    "'parent-lol', and 'parent-know' pairs are concordant. \n",
    "\n",
    "The un-normalized tau coefficient is `C - D`, which is 2 in this case. The normalized version is:\n",
    "\n",
    "$\\tau$ ` = (C - D)/n(n-1)/2`,\n",
    "\n",
    "where `C`(`D`) is the number of concordant(discordant) pairs, and `n` is the length of the ranking list(s). This gives us $\\tau$ `= 0.3`. \n",
    "\n",
    "The `scipy` implementation of this coefficient accounts for ties (multiple entries with the same rank).\n",
    "\n",
    "Let's look at some common test points for the measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self-correlation\n",
    "a = [i for i in range(20)]\n",
    "\n",
    "scipy.stats.kendalltau(a,a).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember that the rows need not be ordered\n",
    "\n",
    "# shuffle in place\n",
    "random.shuffle(a)\n",
    "\n",
    "scipy.stats.kendalltau(a,a).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## anti-correlation\n",
    "a = [i for i in range(20)]\n",
    "b = list(reversed(a))\n",
    "\n",
    "scipy.stats.kendalltau(a,b).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random case\n",
    "# correlation will average 0 and get closer to 0 for large lists\n",
    "a = [i for i in range(1000)]\n",
    "b = random.sample(a, k=len(a))\n",
    "\n",
    "scipy.stats.kendalltau(a,b).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ties\n",
    "\n",
    "# scipy implementation uses:\n",
    "# https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#Tau-b\n",
    "\n",
    "a = [i for i in range(10)]\n",
    "b = [i for i in range(10)]\n",
    "\n",
    "# items in list b at indices 0 and 1 will both have rank 1 (zero-based)\n",
    "b[0] = 1\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "scipy.stats.kendalltau(a,b).correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally: \n",
    "\n",
    "Kendall's tau is not defined for non-conjoint lists, meaning that it won't work for most incomplete lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-biased Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank-biased overlap (RBO) is based on the idea the the overlap (or size of the intersection) of two sets is a good, simple starting point for similarity measures. We apply this to ordinal lists by calculating the overlap at varying depths and cleverly aggregating the results. Importantly, this method does not depend on elements being in both lists.\n",
    "\n",
    "For ordinal lists _S_ and _T_, the _agreement_ (_A_) at depth _k_ is given in terms of the overlap (_X_, the size of the intersection) between _S_ and _T_ at depth _k_.\n",
    "\n",
    "$A_{S,T,k} = \\frac{X_{S,T,k}}{k}$\n",
    "\n",
    "The average overlap for _1_ <= _k_ <= _d_ gives decent similarity measure.\n",
    "\n",
    "If you make it a weighted average and choose your weights to be elements of a geometric series on parameter _p_, you can take d --> infinity and you have a distance measure _r_ bounded by 0 and 1 and controlled by a single parameter, _p_. Values of _p_ close to 1 emphasize agreement between highly ranked elements, while smaller values of _p_ emphasize a broader range of agreement. \n",
    "\n",
    "For truncated lists, you can calculate exactly the minimum (_min_) and maximum value that _r_ can take on, given the unknown values lost in truncation. This is usually quoted in terms of _min_ and the residual difference between the minimum and maximum (_res_). \n",
    "\n",
    "For truncated lists, the base score (_r_) is a function of the cutoff depth (_d_) and can not actually reach 1. We can instead extrapolate from the visible lists and calculate $r_{ext}$ such that it has the range [0-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rbo import rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements in the list can be any object\n",
    "rbo([{'c', 'a'}, 'b', 'd'], ['a', {'c', 'b'}, 'd'], p=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-similarity\n",
    "a = [i for i in range(20)]\n",
    "\n",
    "rbo(a,a,p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order doesn't matter\n",
    "random.shuffle(a)\n",
    "\n",
    "rbo(a,a,p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are comparing ordered lists of objects, not rankings\n",
    "a = [i for i in string.punctuation]\n",
    "\n",
    "rbo(a,a,p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversed case\n",
    "a = [i for i in string.punctuation]\n",
    "b = list(reversed(a))\n",
    "\n",
    "rbo(a,b,p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random comparison\n",
    "a = [i for i in string.punctuation]\n",
    "b = random.sample(a, k=len(a))\n",
    "\n",
    "rbo(a,b,p=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply it!\n",
    "\n",
    "Lesson: most vocabularies on Twitter (1-grams, 2-grams, hashtags) for a location, date, etc. are sparsely populated, meaning that the rankings are largely non-conjoint. When comparing 2 rankings with the similarity measurements described above, it's hard to know when small similarity differences are due to statistics, platform differences, or real textual differences. \n",
    "\n",
    "Two paths forward:\n",
    "\n",
    "* draw random samples from a single corpus to create a distribution for the null hypothesis\n",
    "* create a small vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tweets\n",
    "\n",
    "Let's collect 3 data sets matching 3 keywords: \"mom\", \"dad\", and \"mum\", hypothesizing that a good similarity measurment will be small for \"mom\"-\"mum\" than for \"mom\"-\"dad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get Tweets from the Twitter public API\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Get your Twitter API tokens\n",
    "# this is specific to my computer; modify for yours\n",
    "my_creds_file = '/Users/jkolb/.twitter_api_creds'\n",
    "creds = yaml.load(open(my_creds_file))\n",
    "consumer_key = creds['audience']['consumer_key']\n",
    "consumer_secret = creds['audience']['consumer_secret']\n",
    "access_token_key = creds['audience']['token']\n",
    "access_token_secret = creds['audience']['token_secret']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "api = twitter.Api(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token_key,\n",
    "                 access_token_secret=access_token_secret\n",
    "                 )\n",
    "\n",
    "mom_tweets = []\n",
    "for _ in range(20):\n",
    "    mom_tweets.extend( api.GetSearch(\"mom\",count=100) )\n",
    "    time.sleep(1)\n",
    "\n",
    "dad_tweets = []\n",
    "for _ in range(20):\n",
    "    dad_tweets.extend( api.GetSearch(\"dad\",count=100) )\n",
    "    time.sleep(1)\n",
    "\n",
    "mum_tweets = []\n",
    "for _ in range(20):\n",
    "    mum_tweets.extend( api.GetSearch(\"mom\",count=100) )\n",
    "    time.sleep(1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get Tweets from the Gnip Search API\n",
    "\"\"\"\n",
    "\n",
    "from search.api import Query\n",
    "import json\n",
    "import yaml\n",
    "creds = yaml.load(open('/Users/jkolb/.creds.yaml'))\n",
    "\n",
    "# set up a query to the Gnip Search API\n",
    "q = Query(creds['username'],\n",
    "          creds['password'],\n",
    "          creds['search_endpoint'],\n",
    "          paged=True,\n",
    "          hard_max = 2000, ## <--- control tweet volume here\n",
    "          )\n",
    "\n",
    "# query parameters\n",
    "start_date = '2017-06-01T00:00'\n",
    "end_date = '2017-06-03T00:00'\n",
    "\n",
    "# get the tweet data\n",
    "rule = 'mom'\n",
    "rule += ' -is:retweet'\n",
    "q.execute(rule,start=start_date,end=end_date)\n",
    "mom_tweets = list(q.get_activity_set())\n",
    "\n",
    "rule = 'dad'\n",
    "rule += ' -is:retweet'\n",
    "q.execute(rule,start=start_date,end=end_date)\n",
    "dad_tweets = list(q.get_activity_set())\n",
    "\n",
    "rule = 'mum'\n",
    "rule += ' -is:retweet'\n",
    "q.execute(rule,start=start_date,end=end_date)\n",
    "mum_tweets = list(q.get_activity_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do better n-gram extraction\n",
    "\n",
    "* better stopword list removes unimportant words from list of top-ranked n-grams\n",
    "* better lemmatization and normalization removes duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get tweet bodies\n",
    "dad_bodies = [tweet['body'] for tweet in dad_tweets]\n",
    "mom_bodies = [tweet['body'] for tweet in mom_tweets]\n",
    "mum_bodies = [tweet['body'] for tweet in mum_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a tweet tokenizer and stopword list\n",
    "my_additional_stop_words = ['https','rt']\n",
    "my_additional_stop_words.extend(string.punctuation)\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make vectorizers\n",
    "dad_ngram_vectorizer = text.CountVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )\n",
    "dad_ngram_vectorizer_idf = text.TfidfVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )\n",
    "mom_ngram_vectorizer = text.CountVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )\n",
    "mom_ngram_vectorizer_idf = text.TfidfVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )\n",
    "mum_ngram_vectorizer = text.CountVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )\n",
    "mum_ngram_vectorizer_idf = text.TfidfVectorizer(lowercase=True,\n",
    "                             stop_words=stop_words,\n",
    "                             ngram_range=(1,2),\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             min_df = 2,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def ngram_freq_from_dtmatrix(dtmatrix,col_names):\n",
    "    return dict([(ngram,dtmatrix.getcol(icol).toarray().sum()) for icol,ngram in enumerate(col_names)])\n",
    "def ranked_tuples_from_ngram_freq(term_freq_dict):\n",
    "    return list(reversed(sorted(term_freq_dict.items(),key=operator.itemgetter(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get top ranked ngrams for 'dad' tweets \n",
    "\n",
    "dad_dtmatrix = dad_ngram_vectorizer.fit_transform(dad_bodies)\n",
    "dad_ngrams = dad_ngram_vectorizer.get_feature_names()\n",
    "\n",
    "dad_tf_dict = ngram_freq_from_dtmatrix(dad_dtmatrix,dad_ngrams)\n",
    "dad_ngrams_ranked = ranked_tuples_from_ngram_freq(dad_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get top ranked ngrams for 'mom' tweets \n",
    "\n",
    "mom_dtmatrix = mom_ngram_vectorizer.fit_transform(mom_bodies)\n",
    "mom_ngrams = mom_ngram_vectorizer.get_feature_names()\n",
    "\n",
    "mom_tf_dict = ngram_freq_from_dtmatrix(mom_dtmatrix,mom_ngrams)\n",
    "mom_ngrams_ranked = ranked_tuples_from_ngram_freq(mom_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get top ranked ngrams for 'mum' tweets \n",
    "\n",
    "mum_dtmatrix = mum_ngram_vectorizer.fit_transform(mum_bodies)\n",
    "mum_ngrams = mum_ngram_vectorizer.get_feature_names()\n",
    "\n",
    "mum_tf_dict = ngram_freq_from_dtmatrix(mum_dtmatrix,mum_ngrams)\n",
    "mum_ngrams_ranked = ranked_tuples_from_ngram_freq(mum_tf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "dad_ngrams_ranked[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply by looking at this list, we can see other avenues for improving ngram extraction.\n",
    "\n",
    "* do we include handles?\n",
    "* should we remove RTs?\n",
    "* what about emoji?\n",
    "* minimum token length?\n",
    "\n",
    "For now, we won't go down these paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Kendall's tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## now let's extract the rankings and compare\n",
    "\n",
    "# probably want to cut off the rankings somewhere\n",
    "cutoff = 10000\n",
    "final_cutoff = 300\n",
    "\n",
    "# get the (ngram,rank) lists\n",
    "dad_ngram_ranks = {ngram:rank for rank,(ngram,count) in enumerate(dad_ngrams_ranked[:cutoff])}\n",
    "mom_ngram_ranks = {ngram:rank for rank,(ngram,count) in enumerate(mom_ngrams_ranked[:cutoff])}\n",
    "mum_ngram_ranks = {ngram:rank for rank,(ngram,count) in enumerate(mum_ngrams_ranked[:cutoff])}\n",
    "\n",
    "# get the rank lists\n",
    "# NB: if cutoff lists are not conjoint (they probably aren't), \n",
    "# you'll have to choose one list as a reference\n",
    "dad_ranks = []\n",
    "mom_ranks = []\n",
    "mum_ranks = []\n",
    "\n",
    "data = []\n",
    "for ngram,mom_rank in mom_ngram_ranks.items():\n",
    "    try:\n",
    "        dad_rank = dad_ngram_ranks[ngram]\n",
    "    except KeyError:\n",
    "        # for elements not in list, rank them last\n",
    "        dad_rank = cutoff \n",
    "    try:\n",
    "        # for elements not in list, rank them last\n",
    "        mum_rank = mum_ngram_ranks[ngram]\n",
    "    except KeyError:\n",
    "        mum_rank = cutoff \n",
    "        \n",
    "    if mom_rank < final_cutoff:\n",
    "        dad_ranks.append(dad_rank)\n",
    "        mom_ranks.append(mom_rank)\n",
    "        mum_ranks.append(mum_rank)\n",
    "\n",
    "        data.append((ngram,mom_rank,mum_rank,dad_rank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dad_mom_tau = scipy.stats.kendalltau(dad_ranks,mom_ranks).correlation\n",
    "mum_mom_tau = scipy.stats.kendalltau(mum_ranks,mom_ranks).correlation\n",
    "\n",
    "print('Tau')\n",
    "print('cutoff = ' + str(final_cutoff))\n",
    "print('mom-dad: ' + str(dad_mom_tau))\n",
    "print('mom-mum: ' + str(mum_mom_tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try RBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_top_ngrams = [ngram for ngram,ct in mom_ngrams_ranked][:final_cutoff]\n",
    "mum_top_ngrams = [ngram for ngram,ct in mum_ngrams_ranked][:final_cutoff]\n",
    "dad_top_ngrams = [ngram for ngram,ct in dad_ngrams_ranked][:final_cutoff]\n",
    "\n",
    "mum_mom_rbo = rbo(mom_top_ngrams,mum_top_ngrams,p=0.9)['ext']\n",
    "dad_mom_rbo = rbo(mom_top_ngrams,dad_top_ngrams,p=0.9)['ext']\n",
    "\n",
    "print('RBO')\n",
    "print('cutoff = ' + str(cutoff))\n",
    "print('mom-dad: ' + str(dad_mom_rbo))\n",
    "print('mom-mum: ' + str(mum_mom_rbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "It's hard to interpret similarity scores without carefully selecting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next steps\n",
    "\n",
    "* Find corpora for comparison that have more comparable vocabularies\n",
    "    * \"mom\" vs \"dad\" in bio\n",
    "    * finer slicing to get more similar audiences\n",
    "        * \"mom\" vs \"dad\" in bio for profile location in California\n",
    "    * event driven data: #nbafinals for users from Cleveland vs Bay Area\n",
    "* Make more careful hypothesis tests\n",
    "    * looked at intra-corpus differences to produce a distribution of the measurement under the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
